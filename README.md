# DeterministicFileStoage
A proposed method of file compression and storage that relies on an standard set of incremental files

File paths are an effecient way to compress information and every file can be reconstructed with a collection of hexcode, therefore a program can utilise this in order to have highly space effecient file storage at the cost of much time and file space somewhere else. This is ideal for usage in a server because then multiple parties can point to the same hex chunks and simply back up their list of pointers/file_paths/keys onto something like paper or a relatively small USB drive, which can then reconstruct their desired files given access to the large sequential hexcode. This is ideal for integration with a secure scuttlebutt style gossip network so that bluetooth transfers can be done very quickly with strangers on the street but can still represent comparitively large meaningful payloads but offloading the processing into a time when someone goes home or to the city, wherever the sequential hexcode is stored. I'm sure there are other examples where the transition cost of incredibly high so that this is a good deal to make. 

The set of incremental files would be popular and long sequences of numbers, ones that could easily be generated or found. Some examples include counting, 01234567891011... or the values of transcendental numbers such as pi or e. For example, the value of 91011121314 would be shortened into the 9c11 . That is, the 9th entry on the sequence of 'c'ounting for 11 numbers. This is just the same process as using a psuedo random number generator seed to "store" data. 

The trick to this of course is that it is very computationally intensive to do traditionally. So naturally the method is to just have a neural network that is trained to recognise "where the random data comes from" ( after all, nothing is truly random. There is simply numbers where the origon is known or not ). Which is to say that we will take snippets of numbers, saving the location in the sequence, then the network will classify it, including if it was just typed out and therefore "truly" random. Such a system would have a bias for remembering the early parts of the sequence, but this is more effecient anyway.

This of course can be combined with an optimiser for how long the snippets should be, and even what methods files are best compressed with ( not just which sequence, but traditional compression techniques as well ). A universal file compressor, ineffecient but repeatable. 

